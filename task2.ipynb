{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_wikipedia(url):\n",
    "    # Send a GET request to the URL\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract the title\n",
    "    title = soup.find(id=\"firstHeading\").text\n",
    "    \n",
    "    # Extract the first paragraph (usually a brief description)\n",
    "    description = soup.find('p', class_='').text\n",
    "    \n",
    "    return title, description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the Wikipedia page to scrape\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "\n",
    "# Perform the scraping\n",
    "title, description = scrape_wikipedia(url)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Title: {title}\")\n",
    "print(f\"Description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results to a text file\n",
    "with open('scraping_results.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(f\"Title: {title}\\n\\n\")\n",
    "    f.write(f\"Description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Results have been saved to 'scraping_results.txt'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
